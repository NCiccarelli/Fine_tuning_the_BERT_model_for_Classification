{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ad04628",
   "metadata": {},
   "source": [
    "# Loading the IMDB dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2745152d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\n_cic\\anaconda3\\envs\\intro_to_transformers_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\n_cic\\AppData\\Local\\Temp\\ipykernel_12400\\868218400.py:3: FutureWarning: list_datasets is deprecated and will be removed in the next major version of datasets. Use 'huggingface_hub.list_datasets' instead.\n",
      "  print(len(list_datasets()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53385\n"
     ]
    }
   ],
   "source": [
    "from datasets import list_datasets, load_dataset, list_metrics, load_metric\n",
    "# Print all the available datasets\n",
    "print(len(list_datasets()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3dd87c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('imdb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95b7d5a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b8ea4e",
   "metadata": {},
   "source": [
    "print an example: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48ba145b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"If only to avoid making this type of film in the future. This film is interesting as an experiment but tells no cogent story.<br /><br />One might feel virtuous for sitting thru it because it touches on so many IMPORTANT issues but it does so without any discernable motive. The viewer comes away with no new perspectives (unless one comes up with one while one's mind wanders, as it will invariably do during this pointless film).<br /><br />One might better spend one's time staring out a window at a tree growing.<br /><br />\",\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dba1085d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': Value(dtype='string', id=None),\n",
       " 'label': ClassLabel(names=['neg', 'pos'], id=None)}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'].features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a06d99",
   "metadata": {},
   "source": [
    "# Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6da43801",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"Terrible movie. Nuff Said.<br /><br />These Lines are Just Filler. The movie was bad. Why I have to expand on that I don't know. This is already a waste of my time. I just wanted to warn others. Avoid this movie. The acting sucks and the writing is just moronic. Bad in every way. The only nice thing about the movie are Deniz Akkaya's breasts. Even that was ruined though by a terrible and unneeded rape scene. The movie is a poorly contrived and totally unbelievable piece of garbage.<br /><br />OK now I am just going to rag on IMDb for this stupid rule of 10 lines of text minimum. First I waste my time watching this offal. Then feeling compelled to warn others I create an account with IMDb only to discover that I have to write a friggen essay on the film just to express how bad I think it is. Totally unnecessary.\",\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"imdb\")\n",
    "dataset[\"train\"][100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd0983e",
   "metadata": {},
   "source": [
    "tokenize: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3fab16ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:21<00:00, 2309.14 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "brt_tkn = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "def generate_tokens_for_imdb(examples):\n",
    "    return brt_tkn(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "tkn_datasets = dataset.map(generate_tokens_for_imdb, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6262f55b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tkn_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4aa060",
   "metadata": {},
   "source": [
    "use only 200 data points for training set and test set, as CUDA/GPU is not available: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf7c4e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = tkn_datasets[\"train\"].shuffle(seed=42).select(range(200))\n",
    "evaluation_dataset = tkn_datasets[\"test\"].shuffle(seed=42).select(range(200))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d793996",
   "metadata": {},
   "source": [
    "Load the BERT-based sequence classification model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8906fe49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "mdl = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4cf7c6",
   "metadata": {},
   "source": [
    "Initialize a metric for accuracy measurement using the HuggingFace's datasets library.\n",
    "\n",
    "We define a function, calculate_metrics, that takes model predictions and true labels, computes the class with the highest predicted probability (argmax), and then calculates and returns the accuracy of the predictions compared to the true labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5031870e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install scikit-learn -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3dff2315",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\n_cic\\AppData\\Local\\Temp\\ipykernel_12400\\1633463286.py:3: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  mdl_metrics = load_metric(\"accuracy\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "mdl_metrics = load_metric(\"accuracy\")\n",
    "def calculate_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return mdl_metrics.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75343462",
   "metadata": {},
   "source": [
    "set up training configuration for a model to be trained with the HuggingFace's Trainer class:\n",
    "\n",
    "- the model should be trained for 3 epochs and evaluated after each epoch. \n",
    "- The training and evaluation artifacts will be saved in a directory named \"test_trainer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e686c7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install accelerate -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88f55b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import accelerate\n",
    "\n",
    "from transformers import TrainingArguments, Trainer\n",
    "trng_args = TrainingArguments(output_dir=\"test_trainer\", evaluation_strategy=\"epoch\", num_train_epochs=3, no_cuda=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6d23e2",
   "metadata": {},
   "source": [
    "Instantiate a Trainer object that contains your model, the\n",
    "training arguments, the datasets to be used for training and\n",
    "testing, and the evaluation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "04457002",
   "metadata": {},
   "outputs": [],
   "source": [
    "Mdl_trainer = Trainer(\n",
    "            model=mdl,\n",
    "            args=trng_args,\n",
    "            train_dataset=training_dataset,\n",
    "            eval_dataset=evaluation_dataset,\n",
    "            compute_metrics=calculate_metrics,\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1da1197",
   "metadata": {},
   "source": [
    "Train the model (i.e., fine-tune the model):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3be4299f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\n_cic\\anaconda3\\envs\\intro_to_transformers_env\\lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='75' max='75' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [75/75 43:02, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.690663</td>\n",
       "      <td>0.535000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.673304</td>\n",
       "      <td>0.580000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.636460</td>\n",
       "      <td>0.660000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=75, training_loss=0.6883013407389323, metrics={'train_runtime': 2605.773, 'train_samples_per_second': 0.23, 'train_steps_per_second': 0.029, 'total_flos': 157866633216000.0, 'train_loss': 0.6883013407389323, 'epoch': 3.0})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Mdl_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e0e7b2",
   "metadata": {},
   "source": [
    "Save the fine-tuned trained model locally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d2ff563c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Mdl_trainer.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45180342",
   "metadata": {},
   "source": [
    "We can check the accuracy of the model using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "01186d43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='25' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [25/25 04:13]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** eval metrics *****\n",
      "  epoch                   =        3.0\n",
      "  eval_accuracy           =       0.66\n",
      "  eval_loss               =     0.6365\n",
      "  eval_runtime            = 0:04:24.06\n",
      "  eval_samples_per_second =      0.757\n",
      "  eval_steps_per_second   =      0.095\n"
     ]
    }
   ],
   "source": [
    "metrics = Mdl_trainer.evaluate(evaluation_dataset)\n",
    "Mdl_trainer.log_metrics(\"eval\", metrics)\n",
    "Mdl_trainer.save_metrics(\"eval\", metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7a7a90",
   "metadata": {},
   "source": [
    "# Inference on unseen data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51401bd7",
   "metadata": {},
   "source": [
    "After fine-tuning and saving our model, we can now use it for inference on new data, such as classifying sentiments in IMDB movie reviews.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ac2331",
   "metadata": {},
   "source": [
    "Load the fine-tuned model from the following path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "417d35a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'test_trainer/'\n",
    "md = AutoModelForSequenceClassification.from_pretrained(PATH, local_files_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6969e52",
   "metadata": {},
   "source": [
    "function for inference: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1c206829",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_classification(text):\n",
    "    # Tokenize\n",
    "    inps = brt_tkn(text, padding=True, truncation=True, max_length=512, return_tensors=\"pt\").to(\"cpu\")\n",
    "    # get output\n",
    "    outputs = md(**inps)\n",
    "    # softmax for generating probablities\n",
    "    probabilities = outputs[0].softmax(1)\n",
    "    # get best match.\n",
    "    return probabilities.argmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153ce901",
   "metadata": {},
   "source": [
    "First text for inference: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7df7f56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This show instantly brightens your day. Every character becomes endearing as you watch. Eagerly awaiting season 2.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e322f3e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1)\n"
     ]
    }
   ],
   "source": [
    "print(make_classification(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82050a7",
   "metadata": {},
   "source": [
    "I.e., positive review\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ceecded2",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"The show didn't quite meet my expectations. I regret spending on popcorn, pizza, and burgers. Akshay should stick to comedy; these regal movies better suit actors with a more kingly aura. Overall, it felt like a waste.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3e30c906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0)\n"
     ]
    }
   ],
   "source": [
    "print(make_classification(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5c2fae",
   "metadata": {},
   "source": [
    "I.e., negative review"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "intro_to_transformers_env",
   "language": "python",
   "name": "intro_to_transformers_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
